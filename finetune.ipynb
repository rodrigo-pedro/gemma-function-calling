{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6fKH80TCwoQ"
   },
   "outputs": [],
   "source": [
    "%pip install trl peft bitsandbytes\n",
    "%pip install -U wheel\n",
    "%pip install packaging ninja\n",
    "%pip install flash-attn\n",
    "%pip install --upgrade transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wandb\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qWb3q8anCbxW"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6NZdylFjCiX6"
   },
   "outputs": [],
   "source": [
    "model_name = \"google/gemma-2b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJ95b543CjbL"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_eos_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HJu0A6FCkm0"
   },
   "outputs": [],
   "source": [
    "# We can't use multi line strings for the chat template because newlines and tabs get interpreted. We have to do this wacky string instead.\n",
    "# https://huggingface.co/docs/transformers/main/en/chat_templating#notes-on-whitespace\n",
    "# BOS and EOS automatically added https://huggingface.co/docs/transformers/en/model_doc/gemma#transformers.GemmaTokenizer.bos_token\n",
    "# Uses same prompt format as gemma-2b/7b-it\n",
    "# System prompt part of user prompt, due to the model not having a system prompt\n",
    "chat_template = \\\n",
    "\"{% for message in messages %}\" \\\n",
    "    \"{% if message['from'] == 'system' %}\" \\\n",
    "        \"{{ '<start_of_turn>user\\\\n' + message['value'] }}\" \\\n",
    "    \"{% elif message['from'] == 'human' %}\" \\\n",
    "        \"{% if loop.index0 == 1 %}\" \\\n",
    "            \"{{ '\\\\nUser Question:\\\\n' }}\" \\\n",
    "        \"{% else %}\" \\\n",
    "            \"{{ '<start_of_turn>user\\\\n' }}\" \\\n",
    "        \"{% endif %}\" \\\n",
    "        \"{{ message['value'] + '<end_of_turn>' }}\" \\\n",
    "    \"{% elif message['from'] == 'gpt' %}\" \\\n",
    "        \"{{ '<start_of_turn>model\\\\n'  + message['value'] + ' ' + '<end_of_turn>' }}\" \\\n",
    "    \"{% elif message['from'] == 'function_response' %}\" \\\n",
    "        \"{{ '<start_of_turn>user\\\\n'  + message['value'] + ' ' + '<end_of_turn>' }}\" \\\n",
    "    \"{% endif %}\" \\\n",
    "    \"{% if not loop.last %}\" \\\n",
    "        \"{{ '\\\\n' }}\" \\\n",
    "    \"{% endif %}\" \\\n",
    "\"{% endfor %}\" \\\n",
    "\"{% if not add_generation_prompt is defined %}\" \\\n",
    "    \"{% set add_generation_prompt = false %}\" \\\n",
    "\"{% endif %}\" \\\n",
    "\"{% if add_generation_prompt %}\" \\\n",
    "    \"{{ '\\\\n<start_of_turn>model\\\\n' }}\" \\\n",
    "\"{% endif %}\"\n",
    "\n",
    "tokenizer.chat_template = chat_template\n",
    "\n",
    "dataset_train = load_dataset(\"hypervariance/function-calling-sharegpt\", split=\"train\")\n",
    "dataset_train = dataset_train.map(\n",
    "    lambda x: {\n",
    "        \"formatted_chat\": tokenizer.apply_chat_template(\n",
    "            x[\"conversations\"], tokenize=False,\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "input_ids = tokenizer(\n",
    "     dataset_train[1][\"formatted_chat\"],\n",
    "     return_tensors=\"pt\",\n",
    ")\n",
    "print(tokenizer.decode(input_ids[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9HgjraRHTKt"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ar_rSpJHRX4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"]=\"gemma-2b-function-calling\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, device_map=\"auto\", quantization_config=bnb_config, attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # Disable cache for fine-tuning. We always want to use the most recent values.\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"o_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=8,  # 64\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id =  tokenizer.eos_token_id\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    warmup_ratio=0.03,\n",
    "    max_grad_norm=0.3,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    save_strategy=\"epoch\",  # save at end of epoch. Alternative is \"steps\", which saves at every \"save_steps\"\n",
    "    logging_steps=1,\n",
    "    learning_rate=2e-4,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset_train,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=2048,\n",
    "    dataset_text_field=\"formatted_chat\",\n",
    "    args=training_args,\n",
    "    packing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ehCDrXvO-sMk"
   },
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"gemma-2b-function-calling\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
